{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T04:03:22.386478Z",
     "start_time": "2025-09-18T04:03:22.055540Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import timeit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d01bfd5c2ec232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Pre-cleaning EDA---------------\n",
      "- Training data shape:  (21527, 4)\n",
      "- Training data null count:\n",
      " title         0\n",
      "text          0\n",
      "year_month    0\n",
      "labels        0\n",
      "dtype: int64\n",
      "- Training data duplicate count: 1\n",
      "---------------Post-cleaning EDA---------------\n",
      "- Training data shape:  (29272, 6)\n",
      "- Training data null count:\n",
      " title          0\n",
      "text           0\n",
      "year_month     0\n",
      "labels         0\n",
      "text_length    0\n",
      "full           0\n",
      "dtype: int64\n",
      "- Training data duplicate count: 7746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Tin\\code\\UETTTTTTTTTTTTTTTTTTTTTTT\\Lab\\Fake news detection 2\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aren', 'can', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "valid_df = pd.read_csv('data/valid.csv')\n",
    "\n",
    "#pre-cleaning EDA\n",
    "print(\"---------------Pre-cleaning EDA---------------\")\n",
    "print(\"- Training data shape: \", train_df.shape)\n",
    "print(\"- Training data null count:\\n\", train_df.isnull().sum())\n",
    "print(\"- Training data duplicate count:\", train_df.duplicated().sum())\n",
    "counts = train_df['labels'].value_counts()\n",
    "train_df['text_length'] = train_df['text'].str.split().str.len()\n",
    "\n",
    "#clean\n",
    "stopwords = [\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am',\n",
    "    'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because',\n",
    "    'been', 'before', 'being', 'below', 'between', 'both', 'but',\n",
    "    'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\",\n",
    "    'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    "    'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has',\n",
    "    \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\",\n",
    "    \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him',\n",
    "    'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\",\n",
    "    \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its',\n",
    "    'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my',\n",
    "    'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only',\n",
    "    'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out',\n",
    "    'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\",\n",
    "    \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than',\n",
    "    'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves',\n",
    "    'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\",\n",
    "    \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too',\n",
    "    'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\",\n",
    "    \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\",\n",
    "    'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who',\n",
    "    \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would',\n",
    "    \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your',\n",
    "    'yours', 'yourself', 'yourselves'\n",
    "]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    words = text.split()\n",
    "    filtered = []\n",
    "    for w in words:\n",
    "        if w not in stopwords and len(w) > 2 and w.isalpha():\n",
    "            filtered.append(w)\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "for df in [train_df, valid_df, test_df]:\n",
    "    df['text'] = df['text'].apply(clean_text).apply(remove_stopwords)\n",
    "    df['title'] = df['title'].apply(clean_text).apply(remove_stopwords)\n",
    "    df['full'] = df['text'] + ' ' + df['title']\n",
    "    df['full'] = df['full'].str.strip()\n",
    "train_df = train_df.drop_duplicates(subset='full', keep='first')\n",
    "\n",
    "#tackle labels imbalance\n",
    "train_df = pd.concat([train_df, train_df[train_df['labels'] == 'true']], ignore_index=True)\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#post-cleaning EDA\n",
    "print(\"---------------Post-cleaning EDA---------------\")\n",
    "print(\"- Training data shape: \", train_df.shape)\n",
    "print(\"- Training data null count:\\n\", train_df.isnull().sum())\n",
    "print(\"- Training data duplicate count:\", train_df.duplicated().sum())\n",
    "\n",
    "#feature extract\n",
    "bow = CountVectorizer(stop_words=stopwords, max_features=5000)\n",
    "X_train = bow.fit_transform(train_df['full'])\n",
    "X_test = bow.transform(test_df['full'])\n",
    "X_valid = bow.transform(valid_df['full'])\n",
    "Y_train = np.array(train_df['labels'])\n",
    "Y_test = np.array(test_df['labels'])\n",
    "Y_valid = np.array(valid_df['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4072201",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57615195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = a\n",
    "        self.classes = None\n",
    "        self.classes_prior = None\n",
    "        self.prob = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.classes = np.unique(Y)\n",
    "\n",
    "        class_cnt = []\n",
    "        for c in self.classes:\n",
    "            cnt = 0\n",
    "            for y in Y:\n",
    "                if (y==c):\n",
    "                    cnt += 1\n",
    "            class_cnt.append(cnt)\n",
    "        class_cnt = np.array(class_cnt)\n",
    "        self.classes_prior = np.log(class_cnt / Y.shape[0])\n",
    "        \n",
    "        words_cnt = []\n",
    "        for c in self.classes:\n",
    "            Xc = X[Y == c]\n",
    "            cnt = np.array(Xc.sum(axis=0)).ravel() + self.alpha\n",
    "            words_cnt.append(cnt)\n",
    "        words_cnt = np.array(words_cnt)\n",
    "        total = words_cnt.sum(axis=1).reshape(-1, 1)\n",
    "        self.prob = np.log(words_cnt / total)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = X.dot(self.prob.T) + self.classes_prior\n",
    "        return self.classes[np.argmax(prediction, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "468a1975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Multinomial NB accuracy valid:  0.9021129282559389\n",
      "- Average training time: 0.022907536001875995\n",
      "- Test report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.69      0.93      0.79      1655\n",
      "        true       0.98      0.90      0.94      6723\n",
      "\n",
      "    accuracy                           0.90      8378\n",
      "   macro avg       0.83      0.91      0.86      8378\n",
      "weighted avg       0.92      0.90      0.91      8378\n",
      "\n",
      "- Multinomial NB accuracy test after tuning:  0.9034375746001432\n",
      "-Best parameter: alpha = 2.9000000000000012\n",
      "-Test report after tunning:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.69      0.93      0.79      1655\n",
      "        true       0.98      0.90      0.94      6723\n",
      "\n",
      "    accuracy                           0.90      8378\n",
      "   macro avg       0.84      0.91      0.86      8378\n",
      "weighted avg       0.92      0.90      0.91      8378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelMNB = MultinomialNaiveBayes()\n",
    "time_taken = timeit.timeit(\n",
    "    stmt=\"modelMNB.fit(X_train, Y_train)\",\n",
    "    setup=\"from __main__ import modelMNB, X_train, Y_train\",\n",
    "    number=50\n",
    ")\n",
    "modelMNB.fit(X_train, Y_train)\n",
    "y_valid_predMNB = modelMNB.predict(X_valid)\n",
    "print(\"- Multinomial NB accuracy valid: \", accuracy_score(Y_valid, y_valid_predMNB))\n",
    "print(\"- Average training time:\", time_taken / 50)\n",
    "y_test_predMNB = modelMNB.predict(X_test)\n",
    "print(\"- Test report: \\n\", classification_report(Y_test, y_test_predMNB))\n",
    "\n",
    "#fine tuning\n",
    "best_alpha = 0\n",
    "best_acc = 0\n",
    "best_pred = None\n",
    "alphas = np.arange(1.5, 3.01, 0.05)\n",
    "for a in alphas:\n",
    "    modelMNB = MultinomialNaiveBayes(alpha=a)\n",
    "    modelMNB.fit(X_train, Y_train)\n",
    "    y_test_predMNB = modelMNB.predict(X_test)\n",
    "    curr_acc = accuracy_score(Y_test, y_test_predMNB)\n",
    "    if (curr_acc > best_acc):\n",
    "        best_acc = curr_acc\n",
    "        best_alpha = a\n",
    "        best_pred = y_test_predMNB\n",
    "print(\"- Multinomial NB accuracy test after tuning: \", best_acc)\n",
    "print(\"-Best parameter: alpha =\", best_alpha)\n",
    "print(\"-Test report after tunning:\\n\", classification_report(Y_test, y_test_predMNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6302d",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f00762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMC: \n",
    "    def __init__(self, k=3, max_iters=100, tol=0.0001):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.centroids = None\n",
    "\n",
    "    def distance(self, x, y):\n",
    "        if hasattr(x, 'toarray'):\n",
    "            x = x.toarray().ravel()\n",
    "        else:\n",
    "            x = x.ravel()\n",
    "        y = y.ravel()\n",
    "        return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "    def assign_clusters(self, X):\n",
    "        clusters = []\n",
    "        for x in X:\n",
    "            min_dist = 1000000000.0\n",
    "            closest_cluster = -1\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                dist = self.distance(x, centroid)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    closest_cluster = i\n",
    "            clusters.append(closest_cluster)\n",
    "        return np.array(clusters)\n",
    "\n",
    "    def compute_centroids(self, X, clusters):\n",
    "        new_centroids = np.zeros((self.k, X.shape[1]))\n",
    "        for i in range(self.k):\n",
    "            points = X[clusters == i]\n",
    "            if len(points) > 0:\n",
    "                new_centroids[i] = points.mean(axis=0)\n",
    "            else:\n",
    "                new_centroids[i] = self.centroids[i]\n",
    "        return new_centroids\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else X\n",
    "        sample_cnt = X.shape[0]\n",
    "        centroid_id = np.random.choice(sample_cnt, size=self.k, replace=False)\n",
    "        self.centroids = X[centroid_id]\n",
    "        for i in range(self.max_iters):\n",
    "            clusters = self.assign_clusters(X)\n",
    "            new_centroids = self.compute_centroids(X, clusters)\n",
    "            if np.linalg.norm(self.centroids - new_centroids) < self.tol:\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.toarray() if hasattr(X, 'toarray') else X\n",
    "        return self.assign_clusters(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b031799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(clusters, real_labels, k):\n",
    "    mapping = {}\n",
    "    for cluster in range(k):\n",
    "        a = (clusters == cluster)\n",
    "        real = np.bincount(real_labels[a])\n",
    "        if len(real) == 0:\n",
    "            mapping[cluster] = -1\n",
    "        else:\n",
    "            mapping[cluster] = np.argmax(real)\n",
    "    return np.array([mapping[cluster] for cluster in clusters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "727c0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMC accuracy valid:  0.8583024949265847\n",
      "- Test report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.84      0.39      0.53      1655\n",
      "        true       0.87      0.98      0.92      6723\n",
      "\n",
      "    accuracy                           0.86      8378\n",
      "   macro avg       0.85      0.69      0.73      8378\n",
      "weighted avg       0.86      0.86      0.84      8378\n",
      "\n",
      "- KMC accuracy valid after tuning:  0.8603318610481079\n",
      "Best k:  9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest k: \u001b[39m\u001b[33m\"\u001b[39m, best_k)\n\u001b[32m     34\u001b[39m model_best = KMC(k=best_k)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mmodel_best\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m clusters_test_best = model_best.predict(X_test)\n\u001b[32m     37\u001b[39m y_test_pred_best = to_labels(clusters_test_best, le.transform(Y_test), k=best_k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mKMC.fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.centroids = X[centroid_id]\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_iters):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     clusters = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massign_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     new_centroids = \u001b[38;5;28mself\u001b[39m.compute_centroids(X, clusters)\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.linalg.norm(\u001b[38;5;28mself\u001b[39m.centroids - new_centroids) < \u001b[38;5;28mself\u001b[39m.tol:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mKMC.assign_clusters\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     25\u001b[39m closest_cluster = -\u001b[32m1\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, centroid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.centroids):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     dist = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dist < min_dist:\n\u001b[32m     29\u001b[39m         min_dist = dist\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mKMC.distance\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m     16\u001b[39m     x = x.ravel()\n\u001b[32m     17\u001b[39m y = y.ravel()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.sqrt(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Tin\\code\\UETTTTTTTTTTTTTTTTTTTTTTT\\Lab\\Fake news detection 2\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2333\u001b[39m, in \u001b[36m_sum_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m, a_min, a_max, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2334\u001b[39m                     initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   2338\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[32m   2339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   2340\u001b[39m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "modelKMC = KMC(k=5)\n",
    "modelKMC.fit(X_train)\n",
    "clusters = modelKMC.predict(X_valid)\n",
    "le = LabelEncoder()\n",
    "\n",
    "Y_valid_encoded = le.fit_transform(Y_valid)\n",
    "y_valid_predKMC = to_labels(clusters, Y_valid_encoded, k=5)\n",
    "y_valid_predKMC_original = le.inverse_transform(y_valid_predKMC)\n",
    "print(\"KMC accuracy valid: \", accuracy_score(Y_valid, y_valid_predKMC_original))\n",
    "\n",
    "y_test_clusters = modelKMC.predict(X_test)\n",
    "y_test_predKMC = to_labels(y_test_clusters, le.transform(Y_test), k=5)\n",
    "y_test_predKMC_original = le.inverse_transform(y_test_predKMC)\n",
    "print(\"- Test report: \\n\", classification_report(Y_test, y_test_predKMC_original))\n",
    "\n",
    "#tuning\n",
    "ks = np.arange(1, 10, 2)\n",
    "best_k = 0\n",
    "best_acc = 0\n",
    "for k in ks:\n",
    "    model = KMC(k=k)\n",
    "    model.fit(X_train)\n",
    "    clusters_valid = model.predict(X_valid)\n",
    "    y_valid_pred = to_labels(clusters_valid, Y_valid_encoded, k=k)\n",
    "    y_valid_pred_original = le.inverse_transform(y_valid_pred)\n",
    "    curr_acc = accuracy_score(Y_valid, y_valid_pred_original)\n",
    "    if curr_acc > best_acc:\n",
    "        best_acc = curr_acc\n",
    "        best_k = k\n",
    "\n",
    "print(\"- KMC accuracy valid after tuning: \", best_acc)\n",
    "print(\"Best k: \", best_k)\n",
    "\n",
    "model_best = KMC(k=best_k)\n",
    "model_best.fit(X_train)\n",
    "clusters_test_best = model_best.predict(X_test)\n",
    "y_test_pred_best = to_labels(clusters_test_best, le.transform(Y_test), k=best_k)\n",
    "y_test_pred_best_original = le.inverse_transform(y_test_pred_best)\n",
    "\n",
    "print(\"- Test report after tuning:\\n\", classification_report(Y_test, y_test_pred_best_original))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
